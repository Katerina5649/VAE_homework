{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ViiovXyrqm4M",
        "e4xv32YlnLn-",
        "Q25eGy4cbIUj",
        "KnP0GX_jIz6o",
        "jGQyqu5sJA1M",
        "qfAuJXcZLdK1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Homework\n",
        "\n",
        "In homework you will have three parts:\n",
        "1. Implement sampling for each of the models from the seminar. The code for this part is the same as for the seminar, albeit the VectorQuantizer class for VQ-VAE has a bit modified logic. Follow the sections of this notebook to implement the sampling method for each model. VQ-VAE sampling is tricky, so you might want to leave it till the end of your homework.  \n",
        "\n",
        "2. Code a simple algorithm for manipulations in latent space - see the manual in the corresponding section.\n",
        "\n",
        "3. As a bonus, code convolutional architectures for the models from the seminar.\n",
        "\n",
        "The points for this homework are the following:\n",
        "\n",
        "1. AE Sampling - 2\n",
        "2. VAE Sampling - 2\n",
        "3. RNN latent code generator for VQ-VAE - 4\n",
        "4. VQ-VAE Sampling - 2\n",
        "5. Latent space manipulations - 2\n",
        "6. Bonus - up to 4\n"
      ],
      "metadata": {
        "id": "BiTlf9cKI_Xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "833f0LyXMI7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "rg07rlxwt0ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoOQU5yspfR8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from IPython import display\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from IPython.display import Image\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our new test subjects are human faces from the [lfw dataset](http://vis-www.cs.umass.edu/lfw/)."
      ],
      "metadata": {
        "id": "fRkdlafVps_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from VAE_homework.lfw_dataset import fetch_lfw_dataset\n",
        "data, attrs = fetch_lfw_dataset()"
      ],
      "metadata": {
        "id": "Sx1q-cqjqCdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "TpYVvg8cZBLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = data[:10000].reshape((10000, -1))\n",
        "print(X_train.shape)\n",
        "X_val = data[10000:].reshape((-1, X_train.shape[1]))\n",
        "print(X_val.shape)\n",
        "\n",
        "image_h = data.shape[1]\n",
        "image_w = data.shape[2]"
      ],
      "metadata": {
        "id": "OIEJ15rar_Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalise all images pixels value to be in range  of [0,1]"
      ],
      "metadata": {
        "id": "BUqxYEaL3etO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.float32(X_train)\n",
        "X_train = X_train/255\n",
        "X_val = np.float32(X_val)\n",
        "X_val = X_val/255"
      ],
      "metadata": {
        "id": "Y7Q85Sm1sUEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_gallery(images, h, w, n_row=3, n_col=6):\n",
        "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
        "    plt.figure(figsize=(1.5 * n_col, 1.7 * n_row))\n",
        "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
        "    for i in range(n_row * n_col):\n",
        "        plt.subplot(n_row, n_col, i + 1)\n",
        "        plt.imshow(images[i].reshape((h, w, 3)), cmap=plt.cm.gray, vmin=-1, vmax=1, interpolation='nearest')\n",
        "        plt.xticks(())\n",
        "        plt.yticks(())"
      ],
      "metadata": {
        "id": "D28qhR_vuFv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = data_utils.TensorDataset(torch.Tensor(X_train), torch.zeros(X_train.shape[0],)) # pseudo labels needed to define TensorDataset\n",
        "train_loader = data_utils.DataLoader(train, batch_size=100, shuffle=True)\n",
        "\n",
        "val = data_utils.TensorDataset(torch.Tensor(X_val), torch.zeros(X_val.shape[0],))\n",
        "val_loader = data_utils.DataLoader(val, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "zYSnATLlucTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder"
      ],
      "metadata": {
        "id": "_QXvBdJabNIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "ViiovXyrqm4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember from the lectures, that the simple autoencoder is going to take the input (X), compress it to some hidden (or latent) representation (z) and then reconstruct the original input (X') from it."
      ],
      "metadata": {
        "id": "C0C_J87WuvjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"VAE_homework/Autoencoder_structure.png\")"
      ],
      "metadata": {
        "id": "WqUJumXQ503u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of the autoencoder bottleneck\n",
        "dimZ = 100\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        self.encode = torch.nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size, dimZ),\n",
        "            nn.BatchNorm1d(dimZ),\n",
        "        )\n",
        "\n",
        "        self.decode = torch.nn.Sequential(\n",
        "            nn.Linear(dimZ, hidden_size),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # We can ask to code things here\n",
        "        latent_code = self.encode(x)\n",
        "        reconstruction = self.decode(latent_code)\n",
        "\n",
        "        return reconstruction, latent_code\n",
        "\n",
        "    def sample(self, n_samples):\n",
        "        \"\"\" This is the method you need to code in your Homework\"\"\"\n",
        "        pass"
      ],
      "metadata": {
        "id": "pe4_54AeyMLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelForwardWrapper(nn.Module):\n",
        "    def __init__(self, model, mode):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.mode = mode\n",
        "\n",
        "    def encode(self, input):\n",
        "        if self.mode == \"ae\":\n",
        "            return self.model.encode(input)\n",
        "        elif self.mode == \"vae\":\n",
        "            return self.model.encode(input)[0]\n",
        "        elif self.mode == \"vq-vae\":\n",
        "            return self.model.encode(input)[..., 0, 0]\n",
        "        else:\n",
        "            raise NotImplementedError"
      ],
      "metadata": {
        "id": "ksb0BM2201Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "id": "76gPIJIM0QmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create MSE loss function\n",
        "criterion = torch.nn.MSELoss()\n",
        "autoencoder = Autoencoder(input_size=X_train.shape[1], hidden_size=300).to(device)\n",
        "# Use Adam optimizer\n",
        "opt = optim.Adam(autoencoder.parameters())"
      ],
      "metadata": {
        "id": "1VN0KqIG0KGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []"
      ],
      "metadata": {
        "id": "MZ-67wn70U19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(X_batch, y_batch):\n",
        "    X_batch = X_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "    probs, _ = autoencoder(X_batch)\n",
        "    return F.mse_loss(probs, y_batch)"
      ],
      "metadata": {
        "id": "aDRcoaOs0Y4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5"
      ],
      "metadata": {
        "id": "j9h8QePlJvf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice, that in the training loop the training losses defined for different models are not comparable between each other. However, for the validation loss we are going to use MSE error on the validation set. Thus, we can compare the quality of reconstruction between different models."
      ],
      "metadata": {
        "id": "KiHVW1nmetxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train your autoencoder\n",
        "\n",
        "from typing import Literal\n",
        "def train_model(model, model_type: Literal[\"ae\", \"vae\", \"vq-vae\"]):\n",
        "    for epoch in range(num_epochs):\n",
        "        # In each epoch, we do a full pass over the training data:\n",
        "        start_time = time.time()\n",
        "        model.train(True) # enable dropout / batch_norm training behavior\n",
        "        loss_batch = []\n",
        "        if model_type == \"vq-vae\":\n",
        "            vq_loss_batch = []\n",
        "        for X_batch, _ in train_loader:\n",
        "            # train on batch\n",
        "            if model_type == \"vq-vae\":\n",
        "                recon_loss, vq_loss_ = compute_loss(X_batch, X_batch)\n",
        "                loss = recon_loss + vq_loss_\n",
        "                vq_loss_batch.append(vq_loss_.data.cpu().numpy())\n",
        "            else:\n",
        "                loss = compute_loss(X_batch, X_batch)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "            loss_batch.append(loss.data.cpu().numpy())\n",
        "        train_loss.append(np.mean(loss_batch))\n",
        "        if model_type == \"vq-vae\":\n",
        "            vq_loss.append(np.mean(vq_loss_batch))\n",
        "\n",
        "        model.train(False)\n",
        "\n",
        "        loss_batch = []\n",
        "        for X_batch, _ in val_loader:\n",
        "            if model_type == \"ae\":\n",
        "                probs, _ = model(X_batch.to(device))\n",
        "                x_pred = probs.data.cpu().numpy()\n",
        "            elif model_type == \"vae\":\n",
        "                mu_decoder, _, _, _ = model(X_batch.to(device))\n",
        "                x_pred = mu_decoder.data.cpu().numpy()\n",
        "            elif model_type == \"vq-vae\":\n",
        "                probs, _ = model(X_batch.to(device))\n",
        "                x_pred = probs.data.cpu().numpy()\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "            loss_batch.append(mean_squared_error(X_batch, x_pred))\n",
        "        val_loss.append(np.mean(loss_batch))\n",
        "\n",
        "\n",
        "        # Visualize\n",
        "        display.clear_output(wait=True)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.title(\"Validation loss\")\n",
        "        plt.xlabel(\"#Epoch\")\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.plot(val_loss, 'b',label='val loss')\n",
        "        plt.plot(pd.Series(val_loss).ewm(span=10).mean(),'r',label='ewm val loss')\n",
        "        plt.legend(loc='best')\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "        # Then we print the results for this epoch:\n",
        "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
        "            epoch + 1, num_epochs, time.time() - start_time))\n",
        "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(train_loss[-1]))\n",
        "        if model_type == \"vq-vae\":\n",
        "            print(\"  training VQ loss (in-iteration): \\t{:.6f}\".format(vq_loss[-1]))\n",
        "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_loss[-1]))"
      ],
      "metadata": {
        "id": "QA48ZwJ91AP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(autoencoder, model_type=\"ae\")"
      ],
      "metadata": {
        "id": "EH4CxoWXFmCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, lets examine how well the reconstruction is performed."
      ],
      "metadata": {
        "id": "PUWVn9c13vl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the reconstructions\n",
        "autoencoder.train(False)\n",
        "for j, img in enumerate(val_loader, 0):\n",
        "    inp = img[0].to(device)\n",
        "    pred, _ = autoencoder(inp)\n",
        "    plot_gallery([img[0].numpy(), pred.data.cpu().numpy()], image_h, image_w, n_row=1, n_col=2)\n",
        "    if (j >= 9):\n",
        "        break"
      ],
      "metadata": {
        "id": "FL60G53c1Tw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling (HW) - 2 points"
      ],
      "metadata": {
        "id": "e4xv32YlnLn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After you have implemented sampling method for AE, plot the samples from it here similar to the images above. What do you observe?"
      ],
      "metadata": {
        "id": "5Vd0N9HgnNm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_outputs = autoencoder.sample()\n",
        "plot_gallery(sampled_outputs.data.cpu().numpy(), image_h, image_w, n_row=5, n_col=5)"
      ],
      "metadata": {
        "id": "ttxZrr5UndAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE"
      ],
      "metadata": {
        "id": "Q25eGy4cbIUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "3kIm5FgTqe32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to compare with conventional AE, keep these hyperparameters\n",
        "# or change them for the values that you used before\n",
        "dimZ = 100\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_size=6075, hidden_size=400):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.fc_encode = nn.Linear(input_size, hidden_size)\n",
        "        self.mu_encode = nn.Linear(hidden_size, dimZ)\n",
        "        self.logstd_encode = nn.Linear(hidden_size, dimZ)\n",
        "\n",
        "\n",
        "        self.fc_decode = nn.Linear(dimZ, hidden_size)\n",
        "        self.mu_decode = nn.Linear(hidden_size, input_size)\n",
        "        self.logstd_decode = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.bn_encode = nn.BatchNorm1d(hidden_size)\n",
        "        self.bn_decode = nn.BatchNorm1d(hidden_size)\n",
        "\n",
        "        self.bn_mu_encode = nn.BatchNorm1d(dimZ)\n",
        "\n",
        "    def encode(self, x):\n",
        "        hidden = self.relu(self.bn_encode(self.fc_encode(x)))\n",
        "        return self.bn_mu_encode(self.mu_encode(hidden)), self.logstd_encode(hidden)\n",
        "\n",
        "    def decode(self, z):\n",
        "        hidden = self.relu(self.bn_decode(self.fc_decode(z)))\n",
        "        return self.sigmoid(self.mu_decode(hidden)),\\\n",
        "               self.logstd_decode(hidden)\n",
        "\n",
        "    def gaussian_sampler(self, mu, logsigma):\n",
        "        if self.training:\n",
        "            std = logsigma.exp()\n",
        "            eps = torch.zeros_like(std).normal_()\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu_encoder, logstd_encoder = self.encode(x)\n",
        "        z = self.gaussian_sampler(mu_encoder, logstd_encoder)\n",
        "        mu_decoder, logstd_decoder = self.decode(z)\n",
        "\n",
        "        return mu_decoder, logstd_decoder, mu_encoder, logstd_encoder\n",
        "\n",
        "    def sample(self, n_samples):\n",
        "        \"\"\" This is the method you need to code in your Homework\"\"\"\n",
        "        pass"
      ],
      "metadata": {
        "id": "UkN4KqImchSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def KL_divergence(mu, logsigma):\n",
        "    return - 0.5 * (1 + 2 * logsigma  - mu.pow(2) - logsigma.exp().pow(2)).sum(dim=-1)\n",
        "\n",
        "def log_likelihood(x, mu, logsigma):\n",
        "    return - (logsigma  +\\\n",
        "              (mu - x).pow(2) / logsigma.exp().pow(2) * 0.5).sum(dim=-1)\n",
        "\n",
        "def loss_vae(x, mu_decoder, logstd_decoder, mu_encoder, logstd_encoder):\n",
        "    KL_value = KL_divergence(mu_encoder, logstd_encoder)\n",
        "    log_value = log_likelihood(x, mu_decoder, logstd_decoder)\n",
        "    return - (- 0.5 * KL_value + log_value).mean()"
      ],
      "metadata": {
        "id": "pkYZ5O9beTyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae_autoencoder = VAE(input_size=X_train.shape[1], hidden_size=400).to(device)\n",
        "\n",
        "opt = optim.Adam(vae_autoencoder.parameters())\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "def compute_loss(X_batch, y_batch):\n",
        "    X_batch = X_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "    return loss_vae(X_batch, *vae_autoencoder(X_batch))"
      ],
      "metadata": {
        "id": "yPccLPqteZFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recreate the dataset with smaller batch_size. VAEs are quite picky when it comes to model parameters and architectures\n",
        "\n",
        "train = data_utils.TensorDataset(torch.Tensor(X_train), torch.zeros(X_train.shape[0],))\n",
        "train_loader = data_utils.DataLoader(train, batch_size=100, shuffle=True)\n",
        "\n",
        "val = data_utils.TensorDataset(torch.Tensor(X_val), torch.zeros(X_val.shape[0],))\n",
        "val_loader = data_utils.DataLoader(val, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "tgtKrLQH-XTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(vae_autoencoder, model_type='vae')"
      ],
      "metadata": {
        "id": "G_UkGk3DGLzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = data_utils.DataLoader(val, batch_size=1, shuffle=False)\n",
        "vae_autoencoder.eval()\n",
        "for j, img in enumerate(val_loader, 0):\n",
        "    input = img[0].to(device)\n",
        "    reconstruction_mu, _, _, _ = vae_autoencoder(input)\n",
        "    plot_gallery([img[0].numpy(), reconstruction_mu.data.cpu().numpy()], image_h, image_w, n_row=1, n_col=2)\n",
        "    if (j >= 9):\n",
        "        break"
      ],
      "metadata": {
        "id": "2FFOTYHke9zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling (HW) - 2 points"
      ],
      "metadata": {
        "id": "Uo8chkS3omLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement sampling for VAE similarly to AE.\n",
        "Hint: in the sampling method,\n",
        "- Sample z ~ N(0,1)\n",
        "- Sample from N(decoder_mu(z), decoder_sigma(z))"
      ],
      "metadata": {
        "id": "tT_AoS0eoul3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_outputs = vae_autoencoder.sample()\n",
        "plot_gallery(sampled_outputs.data.cpu().numpy(), image_h, image_w, n_row=5, n_col=5)"
      ],
      "metadata": {
        "id": "9qnnSlBdozJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you observe? Is there any difference w.r.t to an AE?"
      ],
      "metadata": {
        "id": "6wGCAZoxqXnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VQ-VAE"
      ],
      "metadata": {
        "id": "KnP0GX_jIz6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code"
      ],
      "metadata": {
        "id": "zn9xNX7L3n3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous example, we took a continuous normal distribution as a latent prior and posterior distribution.  The authors of the Vector Quantised (VQ) - VAE [link](https://arxiv.org/pdf/1711.00937.pdf) took a step further and imposed discrete (categorical) distributions over prior and posterior. This becomes especially handy when one wants to work with discrete sequences, such as language. This technique is actively used nowadays, for example, in DALL-E.\n",
        "\n",
        "\n",
        "The loss for the VQ-VAE is:\n",
        "\n",
        "\n",
        "$$\n",
        "L = log p(x|z_q(x)) + ||\\ sg[z_e(x)] - e \\ ||^2 + ||\\ z_e(x) - sg[e]\\ || ^2,\n",
        "$$\n",
        "\n",
        "where the first term is the log-likelihood of the data, given quantised embedding, the second term provides learning for the trainable embeddings $e$ and the last term is a regularisation for the encoder embeddings."
      ],
      "metadata": {
        "id": "2HECSEOPsIrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"VAE_homework/va_vae.png\")"
      ],
      "metadata": {
        "id": "Cv8E_IL27hV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "id": "Fjtn9phYv5Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from VAE_homework.nearest_embed import VectorQuantizer\n",
        "\n",
        "class VQ_VAE(nn.Module):\n",
        "    \"\"\"Vector Quantized AutoEncoder\"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    The main parts of VQ_VAE will be the encoder network, decoder network AND\n",
        "    VectorQuantizer class. The encoder and the decoder are fully-connected NN, as in AE or VAE.\n",
        "    The VectorQuantizer is the class that performs quantisation. For each pixel (activation)\n",
        "    in an output of the encoder it:\n",
        "    - Find the nearest neighbour in embedding space\n",
        "    - Substitute the activation with the found embedding.\n",
        "    It then computes the loss part (as in the equation above) AND copies the gradients\n",
        "    (red line in the diagram)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emb_dim=2, input_size=6075, hidden=400, bottleneck=200, k=10, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_size = emb_dim\n",
        "        self.fc1 = nn.Linear(input_size, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, bottleneck)\n",
        "\n",
        "        self.fc3 = nn.Linear(bottleneck, hidden)\n",
        "        self.fc4 = nn.Linear(hidden, input_size)\n",
        "\n",
        "        self.bn_encode = nn.BatchNorm1d(hidden)\n",
        "        self.bn_decode = nn.BatchNorm1d(hidden)\n",
        "        self.bn_encode_2 = nn.BatchNorm1d(bottleneck)\n",
        "\n",
        "        self.emb = VectorQuantizer(num_embeddings=k, embedding_dim=self.emb_size)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.leaky = nn.LeakyReLU()\n",
        "\n",
        "        self.hidden = hidden\n",
        "        self.bottleneck = bottleneck\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"\n",
        "        Will return a tensor of shape [B, bottleneck, 1,1]\n",
        "        \"\"\"\n",
        "        h1 = self.relu(self.bn_encode(self.fc1(x)))\n",
        "        h2 = self.fc2(h1)\n",
        "        return h2[..., None, None]\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = self.relu(self.bn_decode(self.fc3(z)))\n",
        "        return self.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        The pipeline for VQ_VAE is encoder -> quantisation -> decoder.\n",
        "        \"\"\"\n",
        "        z_e = self.encode(x)\n",
        "        quantized_inputs, vq_loss = self.emb(z_e)\n",
        "        return self.decode(quantized_inputs.squeeze(dim=(2,3))), vq_loss\n",
        "\n",
        "    def sample(self, n_samples):\n",
        "        \"\"\" This is the method you need to code in your Homework\"\"\"\n",
        "        pass\n",
        "\n",
        "def loss_vq_vae(x, recon_x, vq_loss, alpha=1.):\n",
        "    reconstruction_loss = F.mse_loss(recon_x, x)\n",
        "    return reconstruction_loss, alpha * vq_loss\n"
      ],
      "metadata": {
        "id": "y2YhjCg3pfLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vq_vae = VQ_VAE(input_size=X_train.shape[1]).to(device)\n",
        "\n",
        "opt = optim.Adam(vq_vae.parameters(), lr=1e-3/2)\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "vq_loss = []\n",
        "\n",
        "def compute_loss(X_batch, y_batch):\n",
        "    X_batch = X_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "    return loss_vq_vae(X_batch, *vq_vae(X_batch))"
      ],
      "metadata": {
        "id": "YFKD4Knmq-5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice, that here we implement the simplest possible VQ-VAE with fully-connected layers and everything being 1-dimensional. In the case of more complex architecture, for example, for images and models with convolutional layers, the latent variables Z could be 2D+ dimensional.\n",
        "\n",
        "In the code above we also only perform reconstruction of images, but we dont sample from VQ-VAE. Sampling from it would require fitting some kind of autoregressive prior on latent variables $z$. In the original paper, the authors used PixelCNN, fitted on latent variables of images to perform sampling. You are going to study these approaches in your homework:)"
      ],
      "metadata": {
        "id": "JHD6103KT4kS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = data_utils.TensorDataset(torch.Tensor(X_train), torch.zeros(X_train.shape[0],)) # pseudo labels needed to define TensorDataset\n",
        "train_loader = data_utils.DataLoader(train, batch_size=100, shuffle=True)\n",
        "\n",
        "val = data_utils.TensorDataset(torch.Tensor(X_val), torch.zeros(X_val.shape[0],))\n",
        "val_loader = data_utils.DataLoader(val, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "NVgX0N7Y_cuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "train_model(vq_vae, model_type='vq-vae')"
      ],
      "metadata": {
        "id": "0MI22QsFu9OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = data_utils.DataLoader(val, batch_size=1, shuffle=False)\n",
        "vq_vae.eval()\n",
        "for j, img in enumerate(val_loader, 0):\n",
        "    input = img[0].to(device)\n",
        "    reconstruction_mu, _ = vq_vae(input)\n",
        "    plot_gallery([img[0].numpy(), reconstruction_mu.data.cpu().numpy()], image_h, image_w, n_row=1, n_col=2)\n",
        "    if (j >= 9):\n",
        "        break"
      ],
      "metadata": {
        "id": "iPvpwYr83ece"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling (HW) - 6 points total: 4 for RNN latent code generator, 2 for the rest"
      ],
      "metadata": {
        "id": "DG_AO3wt3s2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to AE and VAE, implement sampling for VQ-VAE. This task is much more challenging because it will require you to train another model to sample latent variables in an autoregressive way.\n",
        "\n",
        "Firstly, you will need to obtain a set of latent codes, produced by the trained VQ-VAE model. The simplest way of doing that is by applying VQ-VAE to all the images in the training set and saving the corresponding latent codes into tensor (step 1). Then, you can train an autoregressive model on this dataset (step 2). Notice, that you dont require your model to be large. A simple RNN might do the job for the dataset of our size. (4 points)\n",
        "\n",
        "After you have trained the model, you can now sample images from VQ-VAE (steps 3 and 4). (2 points)\n",
        "\n",
        "Here are the steps that will help you to achieve the desired task:\n",
        "\n",
        "1. use trained VQ-VAE to extract latent codes over B number images `latents\n",
        "= vq_vae.encode(images)` -> `encoding_inds, latents = vq_vae.emb.get_encoding_inds(latents[..., None, None])` and reshape them in the format of [B x bottleneck // D] -> `encoding_inds.view([B, vq_vae.bottleneck // vq_vae.emb.D])`.\n",
        "\n",
        "2. You now have B sequences of length bottleneck // D (100 in the example from the seminar). You can train any autoregressive model you have already studied in this lecture series, such as RNN, LSTM or Transformer to predict the next token in this sequence.\n",
        "\n",
        "3. After you have done that, you can now sample any latent code using the trained autoregressive model. Call the `vq_vae.emb.get_quantized_latents(sampled_encoding_inds.view(-1), device)` to obtain `sampled_quantized_latents` and reshape them to the correct size `sampled_quantized_latents.view([B, vq_vae.bottleneck])`\n",
        "\n",
        "4. Finally apply `vq_vae.decode(sampled_quantized_latents)` to get B sampled images!\n",
        "\n",
        "Does it work? Are the results better than for AE or VAE?"
      ],
      "metadata": {
        "id": "Bc5vCW1q374j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_outputs = vq_vae.sample()\n",
        "plot_gallery(sampled_outputs.data.cpu().numpy(), image_h, image_w, n_row=5, n_col=5)"
      ],
      "metadata": {
        "id": "_GP2gnr53y_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manipulations in the latent space - 2 points"
      ],
      "metadata": {
        "id": "jGQyqu5sJA1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are face attributes in our dataset. We're interested in \"Smiling\" column, but feel free to try others as well!\n",
        "\n",
        "The goal of this exercise is to manipulate the vectors in the latent space: find a latent representation of an attribute and then apply it to a random face to see how it changes.\n",
        "\n",
        "1. Extract the \"Smilling\" attribute and create two sets of images: 10 smiling faces and 10 non-smiling ones.\n",
        "\n",
        "2. Compute latent representations for each image in the \"smiling\" set and average those latent vectors. Do the same for the \"non-smiling\" set. You have found a \"vector representation\" of the \"smile\" and \"no smile\" attributes.\n",
        "\n",
        "3. Compute the difference: \"smile\" vector minus \"non-smile\" vector.\n",
        "\n",
        "4. Now check if \"feature arithmetics\" works. Sample a face without a smile, encode it and add the diff from p. 3. Check if it works with AE, VAE and VQ-VAE."
      ],
      "metadata": {
        "id": "jYpd78MeJDwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data, attrs = fetch_lfw_dataset()"
      ],
      "metadata": {
        "id": "2ZiB7K_QJlXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample images\n",
        "smile_size = 10\n",
        "smiling_indeces = ## Your code\n",
        "non_smiling_indeces = ## Your code"
      ],
      "metadata": {
        "id": "4tPrTdHTJ-DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_everybody_happy(net, image):\n",
        "    smile_latent = ## Your code\n",
        "    non_smile_latent = ## Your code\n",
        "\n",
        "    if isinstance(non_smile_latent, tuple):\n",
        "        smile_latent, non_smile_latent = smile_latent[0], non_smile_latent[0]\n",
        "\n",
        "    diff_vector = ## Your code\n",
        "\n",
        "    coeff_images = []\n",
        "    sad_encoded = ## Your code\n",
        "    if isinstance(sad_encoded, tuple):\n",
        "        sad_encoded = sad_encoded[0]\n",
        "\n",
        "    return ## Decoded images with the diff vector applied"
      ],
      "metadata": {
        "id": "l86ciwigKdZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# autoencoder\n",
        "happy_persons = make_everybody_happy(autoencoder, non_smile_image)"
      ],
      "metadata": {
        "id": "IaAkPhruLDiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_col = 6\n",
        "n_row = 2\n",
        "plt.figure(figsize=(1.2 * n_col, 1.5 * n_row))\n",
        "plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
        "for i in range(len(happy_persons)):\n",
        "    # Show a grumpy face and how it is progresses to a happy face!"
      ],
      "metadata": {
        "id": "4o2-e7ERLKtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus - up to 4 points"
      ],
      "metadata": {
        "id": "qfAuJXcZLdK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we have been working with images but only used fully connected networks!\n",
        "\n",
        "Modify the code for AE, VAE and if you dare for VQ-VAE to use convolutions instead."
      ],
      "metadata": {
        "id": "Yq9X5Gi_LfQ2"
      }
    }
  ]
}